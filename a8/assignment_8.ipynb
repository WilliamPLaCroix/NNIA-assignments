{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwL-VcGY6shN"
   },
   "source": [
    "# NNIA Assignment 8\n",
    "**DEADLINE: 12.1.2024 0800 CET**\n",
    "- Name & ID 1 (CMS username):\n",
    "- Name & ID 2 (CMS username):\n",
    "- Hours of work per person:\n",
    "\n",
    "# Submission Instructions\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the submission deadline. All course-related questions can be addressed on the course **CMS Forum**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2-3**. It is fine to submit first **2** assignments without a team, but starting from the **3rd** assignment it is not allowed.\n",
    "* Please include your **names**, **ID's**, **CMS usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (`.zip` is the only accepted extension) in **CMS**.\n",
    "* Only **one** member of the group should make the submission.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2(_Name3_id3).zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization repeatedly students fail to do this.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1 Theory Review (1.5 points)\n",
    "\n",
    "\n",
    "Review [Chapter 7: Regularization for Deep Learning](https://www.deeplearningbook.org/contents/regularization.html) in [Goodfellow et al. (2016)](https://www.deeplearningbook.org/) and answer the following questions:\n",
    "\n",
    "\n",
    "1. Why do we penalize only weights, but not biases? Explain in your own words.\n",
    "2. How does L2 norm affect weights? What feature characteristics are penalized more?\n",
    "3. How does L1 norm treat weights differently than L2 norm? Why can L1 be used as a feature selection mechanism?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1 <font color=\"red\">To Do</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2 Banknote Authentication (7.5 points)\n",
    "We will be working with the [banknote authentication](https://archive.ics.uci.edu/dataset/267/banknote+authentication) dataset. It consists of data extracted from 1372 images of genuine and forged banknotes. The dataset has 4 features and a binary label (0 for authentic and 1 for forgery). The dataset is provided as `data_banknote_authentication.csv` where the last column is the label.\n",
    "\n",
    "### 2.1 Preparing the data (1 point)\n",
    "- Examine the target distribution.\n",
    "    - Is it balanced?\n",
    "    - What would your accuracy be if you guessed randomly?\n",
    "    - Is there anything else we could use as a baseline besides guessing randomly\n",
    "- Split the data randomly into a training and validation set with 80% for training and 20% for validation. You can use [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for this.\n",
    "- You will also need to convert the data to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 <font color=\"red\">To Do</font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.2 Defining a simple neural network (2 points)\n",
    "- Create a simple neural network in pytorch with the following structure:\n",
    "    1. input size: 4\n",
    "    2. first hidden size: 20\n",
    "    3. ReLU\n",
    "    4. second hidden size: 20\n",
    "    5. ReLU\n",
    "    6. output size: 1\n",
    "    7. sigmoid\n",
    "\n",
    "- Train your network on the training data for 1000 epochs using the [Adam Optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with a learning rate of $10^{-4}$. Adam hasn't been covered in lecture yet but its use analogous to SGD. Feed the network all the training at once for each epoch (i.e. don't split the data into minibatches). Since this is a binary classification task you will need to use [BinaryCrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html). Print the loss and accuracy for the training and validation data every 50 epochs.\n",
    "- You do not need to create a Pytorch dataset and dataloader for this task but feel free to do so anyway.\n",
    "- And don't worry, the model is very small so it will train quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.3 Early Stopping & Patience (1 point)\n",
    "- A very common strategy is to use early stoping. But how do we know when to stop? There are several ways which lead to different conditions, most common ones being the combination of (1) patience and (2) minimum improvement in validation. Incorporate these conditions: (1) patience with the constants $3$ and (2) minimum improvement in validation of $10^{-3}$ in your early stopping implementation. That means that you stop training when the validation loss does not decrease for more than $3$ epochs by at least $10^{-3}$ than the previous minimum.\n",
    "- Run your model indefinitely until your model stops. Again print the loss and accuracy for the training and validation data every 50 epochs and print the epoch where your model stopped.\n",
    "- Importantly, at the end report the loss and accuracy on the best model (that is not the last one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.4 Weight Regularization & Visualization (2.5 points)\n",
    "The formulas for regularized loss are given by  $L_1$ and $L_2$ are given by:\n",
    "$$L = L_{0} + \\lambda_{1} \\sum_{i=1}^{n} |w_{i}|$$ for $L1$ and\n",
    "$$L = L_{0} + \\lambda_{2} \\sum_{i=1}^{n} w_{i}^{2}$$ for $L_2$ where $L_{0}$ is the unregularized loss function and $w$ represents the model weights.\n",
    "- Run your model for 1000 epochs:\n",
    "  - With $\\lambda_1 = 0.1$ (coefficient for $L_1$ regularization)\n",
    "  - With $\\lambda_2 = 0.1$ (coefficient for $L_2$ regularization)\n",
    "- Visualize the final weights of the model without any regularization from 2.2 and the two models from this exercise using the provided `visualize_model` function.\n",
    "- What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from visualize_model import visualize_model\n",
    "\n",
    "# example function call\n",
    "# visualize_model(your_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function call should produce an output that looks something like this:\n",
    "\n",
    "![network weights example](weight_visualization_example.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 <font color=\"red\">To Do</font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 $L_2$ regularization and Early stopping (1 point)\n",
    "Early stopping can also be seen as a regularization strategy. In fact, it can even be shown that early stopping is equivalent to adding an $L_2$ regularization term to the loss function with a regularization constant that is inversely proportional to the number of training iterations. This means that stopping the training earlier is similar to using a larger regularization constant, and vice versa.\n",
    "1. Visualize the weights from your early stopping model. How do they compare to those of the L2 model?\n",
    "2. What are the advantages of early stopping over $L_2$ regularization?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 <font color=\"red\">To Do</font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Dataset Augmentation (1 point)\n",
    "Briefly describe in your own words what dataset augmentation is. Provide examples of dataset augmentation techniques and explain why dataset augmentation isn't easily applicable to all tasks.  If you use a source other than [Chapter 7.4: Regularization for Deep Learning](https://www.deeplearningbook.org/contents/regularization.html) in [Goodfellow et al. (2016)](https://www.deeplearningbook.org/) or the lecture slides, please cite it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 <font color=\"red\">To Do</font>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
