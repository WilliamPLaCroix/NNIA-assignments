{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwL-VcGY6shN"
   },
   "source": [
    "# NNIA Assignment 5\n",
    "**DEADLINE: 8.12.2023 0800 CET**\n",
    "- Philipp Hawlitschek 7043167 (phha00002)\n",
    "- William LaCroix 7038732 (wila00001)\n",
    "- Hours of work per person: 3\n",
    "\n",
    "# Submission Instructions\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the submission deadline. All course-related questions can be addressed on the course **CMS Forum**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2-3**. It is fine to submit first **2** assignments without a team, but starting from the **3rd** assignment it is not allowed.\n",
    "* Please include your **names**, **ID's**, **CMS usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (`.zip` is the only accepted extension) in **CMS**.\n",
    "* Only **one** member of the group should make the submission.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2(_Name3_id3).zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization repeatedly students fail to do this.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Calculus Revision (6 points)\n",
    "\n",
    "### 1.1 Critical Points (3 points)\n",
    "\n",
    "Consider the following functions: $f(x)=4x^3 +5x^2 −5$,  $g(x)=\\frac{1}{3}(x−3)^3 +2$ and $h(x)=e^{\\left(2x-2\\right)}\\ \\left(x-2\\right)^{2}$.\n",
    "\n",
    "1. Compute the first derivatives for the three functions. (1 pts)\n",
    "2. Compute the second derivatives for the three functions. (1 pts)\n",
    "3. Find all critical points for the three functions (using first and second derivatives) and give their types (refer to lecture slides). Make sure to comment on how one determines the types of critical points. (2 pt)\n",
    "\n",
    "### 1.2 Local and Global (1 point)\n",
    "\n",
    "![Figure 1](graph.png)\n",
    "\n",
    "Some functions have more than one or two critical points such as the graph of function seen in the above figure.\n",
    "1. Find the critical points of the function in the plot from left to right and name them as $p_1, p_2,...p_n$ (no need for exact coordinates). For each point determine its type and if it is local or global (in the visible range). (0.5)\n",
    "2. Why can local minima cause optimization algorithms to fail? (0.5)\n",
    "\n",
    "### 1.3 Weight Space Symmetry  (2 points)\n",
    "Read section [8.2.2 Local Minima](https://www.deeplearningbook.org/contents/optimization.html) in the deep learning book.\n",
    "1. Briefly explain model identifiability problem and how weight space symmetry relates to it in 3-5 sentences. (1 pt)\n",
    "2. Suppose you had a neural network with a single hidden layer with $m$ neurons and ReLU as the activation function. How many ways of arranging the $m$ hidden neurons are there, which lead to no change in the output layer? (1 pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1\n",
    "1.<br>\n",
    "$f^{'}(x)=12x^2 + 10x$ <br>\n",
    "$g^{'}(x)=(x−3)^2$ <br>\n",
    "$h^{'}(x)=e^{(2x-2)}*2(x-2) + 2e^{(2x-2)}*(x-2)^2$ <br>\n",
    "<br>\n",
    "2.<br>\n",
    "$f^{''}(x) = 24x + 10$ <br>\n",
    "$g^{''}(x)= 2(x-3)$ <br>\n",
    "$h^{''}(x) = e^{(2x-2)}*2 + 4e^{(2x-2)}*(x-2)^2$ <br>\n",
    "<br>\n",
    "3.<br>\n",
    "$f^{'}(x) = 12x^2 + 10x = 0 \\rightarrow x_1 = 0$ the trivial one <br>\n",
    "$f^{'}(x) = 12x^2 + 10x = 0$  factor out 1 x <br>\n",
    "$x(12x + 10) = 0$. Divide both sides of the equation by x <br>\n",
    "$12x + 10 = 0$. Now subtract 10 <br>\n",
    "$12x = -10$. Multiply by $*1/2$ <br>\n",
    "$6x = -5.$ Multiply by $*1/6$ <br>\n",
    "$x = -5/6 \\rightarrow x_2 = -5/6$ <br>\n",
    "<br>\n",
    "$f^{''}(x_1 = 0) = 0 + 10 = 10$. Thus, since $f^{''}(x_1 = 0) > 0$, it is a minimum. <br>\n",
    "$f^{''}(x_2 = -5/6) = 24*(-5/6) + 10 = -10$. Thus, since $f^{''}(x_2 = -5/6) < 0$, it is a maximum. <br>\n",
    "<br>\n",
    "<br>\n",
    "$g^{'}(x)=(x−3)^2 = 0 \\rightarrow x = 3$. Thus, $g(3)$ is the minimum of the parabola.<br>\n",
    "<br>\n",
    "<br>\n",
    "$h^{'}(x)=e^{(2x-2)}*2(x-2) + 2e^{(2x-2)}*(x-2)^2 = 0 \\rightarrow x_1 = 1$, $x_2 = 2$. <br>\n",
    "$h(1)$ is the maximum, $h(2)$ is the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "$p_1$ at $(-3, 0)$ is a local minimum. <br>\n",
    "$p_2$ at $(-2, 4)$ is the global maximum (of the range). <br>\n",
    "$p_3$ at $(2, -5)$ is the global minimum (of the range). <br>\n",
    "$p_4$ at $(3, -1)$ is a local maximum. <br>\n",
    "<br>\n",
    "Local minima can cause optimization algorithms to fail, because these algorithms start at a random point on the curve and try to follow the gradient downwards. They can get stuck in one of the local minima, because the way out of that minimum leads upwards on the curve, which is not possible to pursue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "1. The model identifiability problem means that a model has multiple local minima. A model is said to be identifiable, if one set of model parameters on a sufficiently large enough training set leads to the global minimum, in opposition to any other set of parameters. This is connected to the weight space symmetry, which is a form of nonidentifiability, because swapping out model parameters may lead to the same result.\n",
    "2. For n units and 1 hidden layer, there are $n!$ ways of rearranging the incoming (and outcoming) weight vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural Networks Hello World (4 points)\n",
    "\n",
    "In this exercise you will construct your first neural network using PyTorch. You are provided with a dataset (with train dev spilt) describing the beer consumption for each day for one year in university area of São Paulo, Brazil. The data has 5 features (mean-, min-, max-temperature of the day, precipitation and if it was the weekend) and one target (amount of beer consumed). Your task is to build a neural network to predict the daily beer consumption based on the given features.\n",
    "\n",
    "1. Create a Pytorch Dataset and Dataloader for this data. For the train datatloader use a batch size of 16. For the dev dataloader set the batch size to the length of the dev data (This will make evaluation easier).\n",
    "2. Build a neural network using Pytorch. It should consist of two linear layers with a hidden dimensionality of 32 and ReLU after the first linear layer. For this use [nn.linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) and [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html). As the loss function use [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) and use stochastic gradient descent ([torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) to optimize based on the loss function.\n",
    "3. Train your network on the training data for 20 epochs. After each epoch compute print MSE for the dev set. Use a learning rate of $10^{-3}$.\n",
    "\n",
    "Before you begin, please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html). It should provide a good overview of what you need to build and train a neural network using Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assignment_5_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.data = pd.read_csv(path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        features = torch.tensor(self.data.iloc[index][:5])\n",
    "        target = torch.tensor(self.data.iloc[index][5])\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Temperatura Media ©  Temperatura Minima (C)  Temperatura Maxima (C)   \n",
      "0                  21.70                    18.0                    28.0  \\\n",
      "1                  25.50                    17.6                    30.5   \n",
      "2                  25.66                    20.6                    32.8   \n",
      "3                  24.00                    22.2                    27.7   \n",
      "4                  23.12                    20.6                    28.0   \n",
      "..                   ...                     ...                     ...   \n",
      "287                22.52                    19.3                    28.1   \n",
      "288                22.80                    20.9                    26.9   \n",
      "289                24.04                    20.7                    30.3   \n",
      "290                20.22                    13.5                    30.0   \n",
      "291                19.62                    15.6                    26.2   \n",
      "\n",
      "     Precipitacao (mm)  Final de Semana  Consumo de cerveja (litros)  \n",
      "0                  0.0                0                       27.713  \n",
      "1                 14.1                0                       23.614  \n",
      "2                  4.0                0                       28.617  \n",
      "3                  0.0                0                       22.933  \n",
      "4                  0.1                1                       32.780  \n",
      "..                 ...              ...                          ...  \n",
      "287                0.0                0                       24.388  \n",
      "288                0.0                0                       26.845  \n",
      "289                0.0                0                       30.740  \n",
      "290                0.0                1                       27.518  \n",
      "291                0.0                1                       33.517  \n",
      "\n",
      "[292 rows x 6 columns]\n",
      "(tensor([21.7000, 18.0000, 28.0000,  0.0000,  0.0000], dtype=torch.float64), tensor(27.7130, dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"data/Consumo_cerveja_train.csv\"\n",
    "dev_data_path = \"data/Consumo_cerveja_dev.csv\"\n",
    "\n",
    "train_dataset = Assignment_5_Dataset(train_data_path)\n",
    "dev_dataset = Assignment_5_Dataset(dev_data_path)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=len(dev_dataset), shuffle=True)\n",
    "\n",
    "print(pd.read_csv(train_data_path))\n",
    "print(train_dataloader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create network class\n",
    "\n",
    "class Assignment_5_Network(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super(Assignment_5_Network, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.linear1 = nn.Linear(5, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.linear1.weight.data = self.linear1.weight.data.to(torch.float64)\n",
    "        self.linear1.bias.data = self.linear1.bias.data.to(torch.float64)\n",
    "        self.linear2.weight.data = self.linear2.weight.data.to(torch.float64)\n",
    "        self.linear2.bias.data = self.linear2.bias.data.to(torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def optimize(self, dataTrain, dataDev, epochs):\n",
    "        loss_function = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), self.lr)\n",
    "        \n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            for features, target in dataTrain:\n",
    "                optimizer.zero_grad()\n",
    "                predict = self(features)\n",
    "                loss = loss_function(predict, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for features_dev, targets_dev in dataDev:\n",
    "                    outputs_dev = self(features_dev)\n",
    "                    dev_loss = loss_function(outputs_dev, targets_dev)\n",
    "\n",
    "            # Print MSE loss\n",
    "            print(f\"Dev loss in epoch [{epoch+1}/{epochs}]: {dev_loss.item()}\")\n",
    "            total_loss += dev_loss.item()\n",
    "        \n",
    "        print(\"\")\n",
    "        print(f\"Average loss per epoch: {total_loss/epochs}\")\n",
    "        return total_loss/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/pj/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/pj/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([73])) that is different to the input size (torch.Size([73, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss in epoch [1/20]: 587.1702720039407\n",
      "Dev loss in epoch [2/20]: 573.6397017984303\n",
      "Dev loss in epoch [3/20]: 547.1178999789487\n",
      "Dev loss in epoch [4/20]: 113.64783596768568\n",
      "Dev loss in epoch [5/20]: 496.96151572675956\n",
      "Dev loss in epoch [6/20]: 461.3656259451122\n",
      "Dev loss in epoch [7/20]: 428.3850628161838\n",
      "Dev loss in epoch [8/20]: 397.9781734749491\n",
      "Dev loss in epoch [9/20]: 369.6690712176534\n",
      "Dev loss in epoch [10/20]: 343.2656573405851\n",
      "Dev loss in epoch [11/20]: 319.0078912347184\n",
      "Dev loss in epoch [12/20]: 296.41087377702917\n",
      "Dev loss in epoch [13/20]: 275.50884762647246\n",
      "Dev loss in epoch [14/20]: 256.3222015634188\n",
      "Dev loss in epoch [15/20]: 238.56874645589855\n",
      "Dev loss in epoch [16/20]: 221.9833084381833\n",
      "Dev loss in epoch [17/20]: 206.7931678044394\n",
      "Dev loss in epoch [18/20]: 192.7912075754887\n",
      "Dev loss in epoch [19/20]: 179.7175411665699\n",
      "Dev loss in epoch [20/20]: 167.51087023941602\n",
      "\n",
      "Average loss per epoch: 333.6907736075942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "333.6907736075942"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO train network and print dev MSE\n",
    "\n",
    "network = Assignment_5_Network(0.001)\n",
    "network.optimize(train_dataloader, dev_dataloader, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Learning Rate Effect on Performance (1 point)\n",
    "\n",
    "Run your neural network with different learning rates (scale logarithmically from $10^{-10}$ to $10^{0}$) and observe the effects on performance. Importantly keep the number of optimization steps (epochs) constant. Because the process is very random run the model multiple times (e.g. 3 times) and plot the results. Try to form hypothesis about the results which you got.\n",
    "\n",
    "Don't worry, the model training can be done on a CPU and is not computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1e-10:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 457.997672293108\n",
      "Dev loss in epoch [2/20]: 457.99243896934644\n",
      "Dev loss in epoch [3/20]: 457.98725079783685\n",
      "Dev loss in epoch [4/20]: 457.9820164923582\n",
      "Dev loss in epoch [5/20]: 457.9768067289819\n",
      "Dev loss in epoch [6/20]: 457.9715406865178\n",
      "Dev loss in epoch [7/20]: 457.9663886821533\n",
      "Dev loss in epoch [8/20]: 457.96120553922054\n",
      "Dev loss in epoch [9/20]: 457.95602301231634\n",
      "Dev loss in epoch [10/20]: 457.9507773912159\n",
      "Dev loss in epoch [11/20]: 457.94556031724693\n",
      "Dev loss in epoch [12/20]: 457.9403384130039\n",
      "Dev loss in epoch [13/20]: 457.9351261705451\n",
      "Dev loss in epoch [14/20]: 457.9299170894967\n",
      "Dev loss in epoch [15/20]: 457.92471758711815\n",
      "Dev loss in epoch [16/20]: 457.9195578060773\n",
      "Dev loss in epoch [17/20]: 457.9143942347262\n",
      "Dev loss in epoch [18/20]: 457.90921574900204\n",
      "Dev loss in epoch [19/20]: 457.904035741704\n",
      "Dev loss in epoch [20/20]: 457.8988741526508\n",
      "\n",
      "Average loss per epoch: 457.9481928927312\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1e-10:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 524.3235284648982\n",
      "Dev loss in epoch [2/20]: 524.3178719291883\n",
      "Dev loss in epoch [3/20]: 524.3122484364375\n",
      "Dev loss in epoch [4/20]: 524.3065499538576\n",
      "Dev loss in epoch [5/20]: 524.3008895489835\n",
      "Dev loss in epoch [6/20]: 524.2951681101714\n",
      "Dev loss in epoch [7/20]: 524.2894802781344\n",
      "Dev loss in epoch [8/20]: 524.2837435683439\n",
      "Dev loss in epoch [9/20]: 524.2780496208186\n",
      "Dev loss in epoch [10/20]: 524.2723714984138\n",
      "Dev loss in epoch [11/20]: 524.2666210785636\n",
      "Dev loss in epoch [12/20]: 524.2610000869663\n",
      "Dev loss in epoch [13/20]: 524.2553776934922\n",
      "Dev loss in epoch [14/20]: 524.2497130246508\n",
      "Dev loss in epoch [15/20]: 524.2441043430638\n",
      "Dev loss in epoch [16/20]: 524.238460824772\n",
      "Dev loss in epoch [17/20]: 524.2327416482598\n",
      "Dev loss in epoch [18/20]: 524.2271390258863\n",
      "Dev loss in epoch [19/20]: 524.2214671253791\n",
      "Dev loss in epoch [20/20]: 524.2158398876687\n",
      "\n",
      "Average loss per epoch: 524.2696183073974\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1e-10:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 588.0663059947289\n",
      "Dev loss in epoch [2/20]: 588.059219036907\n",
      "Dev loss in epoch [3/20]: 588.0521584834481\n",
      "Dev loss in epoch [4/20]: 588.0450384818504\n",
      "Dev loss in epoch [5/20]: 588.0379694342776\n",
      "Dev loss in epoch [6/20]: 588.0309026016286\n",
      "Dev loss in epoch [7/20]: 588.0238187896836\n",
      "Dev loss in epoch [8/20]: 588.016732185029\n",
      "Dev loss in epoch [9/20]: 588.0096563422961\n",
      "Dev loss in epoch [10/20]: 588.0025799109212\n",
      "Dev loss in epoch [11/20]: 587.9955431102471\n",
      "Dev loss in epoch [12/20]: 587.9884524387853\n",
      "Dev loss in epoch [13/20]: 587.9813425000092\n",
      "Dev loss in epoch [14/20]: 587.9742677242892\n",
      "Dev loss in epoch [15/20]: 587.9671382389812\n",
      "Dev loss in epoch [16/20]: 587.960051594031\n",
      "Dev loss in epoch [17/20]: 587.952916504426\n",
      "Dev loss in epoch [18/20]: 587.9458815009871\n",
      "Dev loss in epoch [19/20]: 587.9388282736562\n",
      "Dev loss in epoch [20/20]: 587.9318037893668\n",
      "\n",
      "Average loss per epoch: 587.9990303467773\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1e-09:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 397.98563314771945\n",
      "Dev loss in epoch [2/20]: 397.91413002810594\n",
      "Dev loss in epoch [3/20]: 397.84300736583054\n",
      "Dev loss in epoch [4/20]: 397.77084041463155\n",
      "Dev loss in epoch [5/20]: 397.69980924083944\n",
      "Dev loss in epoch [6/20]: 397.62870065534474\n",
      "Dev loss in epoch [7/20]: 397.5571361726914\n",
      "Dev loss in epoch [8/20]: 397.48642758989604\n",
      "Dev loss in epoch [9/20]: 397.41546326195504\n",
      "Dev loss in epoch [10/20]: 397.34526595089926\n",
      "Dev loss in epoch [11/20]: 397.2738296484065\n",
      "Dev loss in epoch [12/20]: 397.203289946919\n",
      "Dev loss in epoch [13/20]: 397.13204526299836\n",
      "Dev loss in epoch [14/20]: 397.0611672535247\n",
      "Dev loss in epoch [15/20]: 396.9901514362261\n",
      "Dev loss in epoch [16/20]: 396.9190731042308\n",
      "Dev loss in epoch [17/20]: 396.84746193966976\n",
      "Dev loss in epoch [18/20]: 396.77672728815577\n",
      "Dev loss in epoch [19/20]: 396.7056569480519\n",
      "Dev loss in epoch [20/20]: 396.6344315350872\n",
      "\n",
      "Average loss per epoch: 397.3095124095591\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1e-09:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 757.0472451062285\n",
      "Dev loss in epoch [2/20]: 756.9585382015074\n",
      "Dev loss in epoch [3/20]: 756.8701418229798\n",
      "Dev loss in epoch [4/20]: 756.7824153235528\n",
      "Dev loss in epoch [5/20]: 756.6943505027672\n",
      "Dev loss in epoch [6/20]: 756.6064233041842\n",
      "Dev loss in epoch [7/20]: 756.5184212366833\n",
      "Dev loss in epoch [8/20]: 756.4294198200732\n",
      "Dev loss in epoch [9/20]: 756.3408481942658\n",
      "Dev loss in epoch [10/20]: 756.2517881159775\n",
      "Dev loss in epoch [11/20]: 756.1646485449863\n",
      "Dev loss in epoch [12/20]: 756.0771522294124\n",
      "Dev loss in epoch [13/20]: 755.9890930947932\n",
      "Dev loss in epoch [14/20]: 755.9003397530043\n",
      "Dev loss in epoch [15/20]: 755.8112487417577\n",
      "Dev loss in epoch [16/20]: 755.7231163310689\n",
      "Dev loss in epoch [17/20]: 755.6347401258857\n",
      "Dev loss in epoch [18/20]: 755.546502513125\n",
      "Dev loss in epoch [19/20]: 755.4581985502168\n",
      "Dev loss in epoch [20/20]: 755.3693341300532\n",
      "\n",
      "Average loss per epoch: 756.2086982821263\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1e-09:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 670.0686095997936\n",
      "Dev loss in epoch [2/20]: 669.9689473550258\n",
      "Dev loss in epoch [3/20]: 669.8677120691664\n",
      "Dev loss in epoch [4/20]: 669.7679370011141\n",
      "Dev loss in epoch [5/20]: 669.6682346770615\n",
      "Dev loss in epoch [6/20]: 669.5690842374548\n",
      "Dev loss in epoch [7/20]: 669.4689121056017\n",
      "Dev loss in epoch [8/20]: 669.3688930792201\n",
      "Dev loss in epoch [9/20]: 669.269058463207\n",
      "Dev loss in epoch [10/20]: 669.1699580171305\n",
      "Dev loss in epoch [11/20]: 669.0712466865884\n",
      "Dev loss in epoch [12/20]: 668.9717981837549\n",
      "Dev loss in epoch [13/20]: 668.8720012409296\n",
      "Dev loss in epoch [14/20]: 668.7715241134676\n",
      "Dev loss in epoch [15/20]: 668.6720007077836\n",
      "Dev loss in epoch [16/20]: 668.5726402086202\n",
      "Dev loss in epoch [17/20]: 668.4730262340804\n",
      "Dev loss in epoch [18/20]: 668.3740718868323\n",
      "Dev loss in epoch [19/20]: 668.2740629157329\n",
      "Dev loss in epoch [20/20]: 668.1741616188358\n",
      "\n",
      "Average loss per epoch: 669.1206940200701\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1e-08:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 400.2696976772144\n",
      "Dev loss in epoch [2/20]: 399.7383312625065\n",
      "Dev loss in epoch [3/20]: 399.20758150558714\n",
      "Dev loss in epoch [4/20]: 398.67646863354923\n",
      "Dev loss in epoch [5/20]: 398.14487957296205\n",
      "Dev loss in epoch [6/20]: 397.61636892360724\n",
      "Dev loss in epoch [7/20]: 397.0886838939337\n",
      "Dev loss in epoch [8/20]: 396.55833920388415\n",
      "Dev loss in epoch [9/20]: 396.0286160829897\n",
      "Dev loss in epoch [10/20]: 395.5021854886325\n",
      "Dev loss in epoch [11/20]: 394.9757309574494\n",
      "Dev loss in epoch [12/20]: 394.44771605095696\n",
      "Dev loss in epoch [13/20]: 393.917008696998\n",
      "Dev loss in epoch [14/20]: 393.38733933246385\n",
      "Dev loss in epoch [15/20]: 392.86391307158607\n",
      "Dev loss in epoch [16/20]: 392.34036795770965\n",
      "Dev loss in epoch [17/20]: 391.8206359819179\n",
      "Dev loss in epoch [18/20]: 391.29852352402384\n",
      "Dev loss in epoch [19/20]: 390.7741528372083\n",
      "Dev loss in epoch [20/20]: 390.254045073427\n",
      "\n",
      "Average loss per epoch: 395.2455292864304\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1e-08:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 692.1390031305939\n",
      "Dev loss in epoch [2/20]: 691.5691406726984\n",
      "Dev loss in epoch [3/20]: 690.9953594382273\n",
      "Dev loss in epoch [4/20]: 690.4242147301333\n",
      "Dev loss in epoch [5/20]: 689.8540432979154\n",
      "Dev loss in epoch [6/20]: 689.2904239444891\n",
      "Dev loss in epoch [7/20]: 688.7166624778324\n",
      "Dev loss in epoch [8/20]: 688.1476275489365\n",
      "Dev loss in epoch [9/20]: 687.5815761889729\n",
      "Dev loss in epoch [10/20]: 687.017126872814\n",
      "Dev loss in epoch [11/20]: 686.4495645433201\n",
      "Dev loss in epoch [12/20]: 685.8904938170664\n",
      "Dev loss in epoch [13/20]: 685.3336129416895\n",
      "Dev loss in epoch [14/20]: 684.7715971107614\n",
      "Dev loss in epoch [15/20]: 684.2032349264529\n",
      "Dev loss in epoch [16/20]: 683.643242450083\n",
      "Dev loss in epoch [17/20]: 683.0820919024179\n",
      "Dev loss in epoch [18/20]: 682.5275449313967\n",
      "Dev loss in epoch [19/20]: 681.9602222368837\n",
      "Dev loss in epoch [20/20]: 681.4011978719451\n",
      "\n",
      "Average loss per epoch: 686.7498990517313\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1e-08:\n",
      "##############################\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss in epoch [1/20]: 820.4557128426301\n",
      "Dev loss in epoch [2/20]: 819.4100292549656\n",
      "Dev loss in epoch [3/20]: 818.375548056788\n",
      "Dev loss in epoch [4/20]: 817.3319021490612\n",
      "Dev loss in epoch [5/20]: 816.285416085269\n",
      "Dev loss in epoch [6/20]: 815.2422955287725\n",
      "Dev loss in epoch [7/20]: 814.2117711579828\n",
      "Dev loss in epoch [8/20]: 813.1649950429771\n",
      "Dev loss in epoch [9/20]: 812.1253266032779\n",
      "Dev loss in epoch [10/20]: 811.0948914786352\n",
      "Dev loss in epoch [11/20]: 810.0663636324731\n",
      "Dev loss in epoch [12/20]: 809.0352690804489\n",
      "Dev loss in epoch [13/20]: 807.9948838537922\n",
      "Dev loss in epoch [14/20]: 806.9631356630068\n",
      "Dev loss in epoch [15/20]: 805.9384982180529\n",
      "Dev loss in epoch [16/20]: 804.9144307267635\n",
      "Dev loss in epoch [17/20]: 803.8811632273834\n",
      "Dev loss in epoch [18/20]: 802.8523089713724\n",
      "Dev loss in epoch [19/20]: 801.8300602493015\n",
      "Dev loss in epoch [20/20]: 800.8064427733799\n",
      "\n",
      "Average loss per epoch: 810.5990222298167\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1e-07:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 565.0280623800453\n",
      "Dev loss in epoch [2/20]: 559.2039927610327\n",
      "Dev loss in epoch [3/20]: 553.5631810820944\n",
      "Dev loss in epoch [4/20]: 548.0044793756532\n",
      "Dev loss in epoch [5/20]: 542.5072422659891\n",
      "Dev loss in epoch [6/20]: 537.0894857203091\n",
      "Dev loss in epoch [7/20]: 531.6704997397393\n",
      "Dev loss in epoch [8/20]: 526.3756471464292\n",
      "Dev loss in epoch [9/20]: 521.1358093623884\n",
      "Dev loss in epoch [10/20]: 515.9049268530357\n",
      "Dev loss in epoch [11/20]: 510.7485361993576\n",
      "Dev loss in epoch [12/20]: 505.60178900074993\n",
      "Dev loss in epoch [13/20]: 500.472764773721\n",
      "Dev loss in epoch [14/20]: 495.4582159908474\n",
      "Dev loss in epoch [15/20]: 490.45336933399494\n",
      "Dev loss in epoch [16/20]: 485.4670440720865\n",
      "Dev loss in epoch [17/20]: 480.58019281801967\n",
      "Dev loss in epoch [18/20]: 475.7344828355078\n",
      "Dev loss in epoch [19/20]: 470.94253743405363\n",
      "Dev loss in epoch [20/20]: 466.21901564065524\n",
      "\n",
      "Average loss per epoch: 514.1080637392854\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1e-07:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 534.3142799768704\n",
      "Dev loss in epoch [2/20]: 530.1324156848293\n",
      "Dev loss in epoch [3/20]: 525.9957549598423\n",
      "Dev loss in epoch [4/20]: 521.8581660646857\n",
      "Dev loss in epoch [5/20]: 517.7632917881613\n",
      "Dev loss in epoch [6/20]: 513.6956990619826\n",
      "Dev loss in epoch [7/20]: 509.7091663849468\n",
      "Dev loss in epoch [8/20]: 505.6929457787308\n",
      "Dev loss in epoch [9/20]: 501.7492461872879\n",
      "Dev loss in epoch [10/20]: 497.83522342407497\n",
      "Dev loss in epoch [11/20]: 493.91062514035394\n",
      "Dev loss in epoch [12/20]: 490.0148303848177\n",
      "Dev loss in epoch [13/20]: 486.2105369968698\n",
      "Dev loss in epoch [14/20]: 482.4031843844953\n",
      "Dev loss in epoch [15/20]: 478.6367676860164\n",
      "Dev loss in epoch [16/20]: 474.84374285832854\n",
      "Dev loss in epoch [17/20]: 471.1059114717076\n",
      "Dev loss in epoch [18/20]: 467.4322977578547\n",
      "Dev loss in epoch [19/20]: 463.74487690907864\n",
      "Dev loss in epoch [20/20]: 460.093966293903\n",
      "\n",
      "Average loss per epoch: 496.35714645974184\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1e-07:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 557.3953817876613\n",
      "Dev loss in epoch [2/20]: 551.9277201340651\n",
      "Dev loss in epoch [3/20]: 546.539953226843\n",
      "Dev loss in epoch [4/20]: 541.2320864343739\n",
      "Dev loss in epoch [5/20]: 535.9250721917674\n",
      "Dev loss in epoch [6/20]: 530.70940512678\n",
      "Dev loss in epoch [7/20]: 525.5534366609941\n",
      "Dev loss in epoch [8/20]: 520.4547748572927\n",
      "Dev loss in epoch [9/20]: 515.4022193973235\n",
      "Dev loss in epoch [10/20]: 510.3658475521274\n",
      "Dev loss in epoch [11/20]: 505.40325810798805\n",
      "Dev loss in epoch [12/20]: 500.47067420655867\n",
      "Dev loss in epoch [13/20]: 495.60605886666974\n",
      "Dev loss in epoch [14/20]: 490.8052771225405\n",
      "Dev loss in epoch [15/20]: 486.02957269203614\n",
      "Dev loss in epoch [16/20]: 481.2995942165716\n",
      "Dev loss in epoch [17/20]: 476.6220237131018\n",
      "Dev loss in epoch [18/20]: 471.9681588769734\n",
      "Dev loss in epoch [19/20]: 467.33454943077714\n",
      "Dev loss in epoch [20/20]: 462.8105701512403\n",
      "\n",
      "Average loss per epoch: 508.6927817376842\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1e-06:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 1206.782856797333\n",
      "Dev loss in epoch [2/20]: 1057.8417246033223\n",
      "Dev loss in epoch [3/20]: 931.4185675646635\n",
      "Dev loss in epoch [4/20]: 823.2580706190798\n",
      "Dev loss in epoch [5/20]: 730.6656637935445\n",
      "Dev loss in epoch [6/20]: 648.7588030428548\n",
      "Dev loss in epoch [7/20]: 576.4102876378823\n",
      "Dev loss in epoch [8/20]: 513.5007308040646\n",
      "Dev loss in epoch [9/20]: 456.82387860729534\n",
      "Dev loss in epoch [10/20]: 406.568003821412\n",
      "Dev loss in epoch [11/20]: 362.03707674655\n",
      "Dev loss in epoch [12/20]: 322.7138556088043\n",
      "Dev loss in epoch [13/20]: 287.42459564481345\n",
      "Dev loss in epoch [14/20]: 255.99804598637556\n",
      "Dev loss in epoch [15/20]: 228.286416462335\n",
      "Dev loss in epoch [16/20]: 203.79642373583627\n",
      "Dev loss in epoch [17/20]: 182.25660545297086\n",
      "Dev loss in epoch [18/20]: 163.19455079287326\n",
      "Dev loss in epoch [19/20]: 146.6889026071488\n",
      "Dev loss in epoch [20/20]: 132.13984086903614\n",
      "\n",
      "Average loss per epoch: 481.82824505990993\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1e-06:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 705.5600903306171\n",
      "Dev loss in epoch [2/20]: 614.3637925384185\n",
      "Dev loss in epoch [3/20]: 535.7149899469558\n",
      "Dev loss in epoch [4/20]: 467.1581317802304\n",
      "Dev loss in epoch [5/20]: 407.8904405644841\n",
      "Dev loss in epoch [6/20]: 355.77754670164586\n",
      "Dev loss in epoch [7/20]: 310.553045565006\n",
      "Dev loss in epoch [8/20]: 270.5758680467357\n",
      "Dev loss in epoch [9/20]: 236.50027734066052\n",
      "Dev loss in epoch [10/20]: 206.9145247478802\n",
      "Dev loss in epoch [11/20]: 181.05797987388002\n",
      "Dev loss in epoch [12/20]: 159.02125605187192\n",
      "Dev loss in epoch [13/20]: 139.7646615330292\n",
      "Dev loss in epoch [14/20]: 123.40163646190973\n",
      "Dev loss in epoch [15/20]: 109.38488423532529\n",
      "Dev loss in epoch [16/20]: 97.41721227400143\n",
      "Dev loss in epoch [17/20]: 87.19208191066282\n",
      "Dev loss in epoch [18/20]: 78.50198181444615\n",
      "Dev loss in epoch [19/20]: 71.20942088576663\n",
      "Dev loss in epoch [20/20]: 65.14877961325223\n",
      "\n",
      "Average loss per epoch: 261.15543011083906\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1e-06:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 377.6604500041765\n",
      "Dev loss in epoch [2/20]: 338.48642745436587\n",
      "Dev loss in epoch [3/20]: 302.78728146274636\n",
      "Dev loss in epoch [4/20]: 270.85009970477495\n",
      "Dev loss in epoch [5/20]: 242.47711317564992\n",
      "Dev loss in epoch [6/20]: 216.66106147465882\n",
      "Dev loss in epoch [7/20]: 193.9189186008774\n",
      "Dev loss in epoch [8/20]: 173.63717401283336\n",
      "Dev loss in epoch [9/20]: 155.49065767980676\n",
      "Dev loss in epoch [10/20]: 139.52369577674915\n",
      "Dev loss in epoch [11/20]: 125.28339272905234\n",
      "Dev loss in epoch [12/20]: 112.80010829128372\n",
      "Dev loss in epoch [13/20]: 101.82478699172792\n",
      "Dev loss in epoch [14/20]: 92.29311111778959\n",
      "Dev loss in epoch [15/20]: 84.09320895030356\n",
      "Dev loss in epoch [16/20]: 76.93367778913591\n",
      "Dev loss in epoch [17/20]: 70.75315171442433\n",
      "Dev loss in epoch [18/20]: 65.34505129675227\n",
      "Dev loss in epoch [19/20]: 60.86002961561785\n",
      "Dev loss in epoch [20/20]: 56.872364014376416\n",
      "\n",
      "Average loss per epoch: 162.9275880928551\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1e-05:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 276.1502445713933\n",
      "Dev loss in epoch [2/20]: 85.93943918701284\n",
      "Dev loss in epoch [3/20]: 47.287498946320206\n",
      "Dev loss in epoch [4/20]: 39.96211633880993\n",
      "Dev loss in epoch [5/20]: 38.14696189764622\n",
      "Dev loss in epoch [6/20]: 37.08185310590681\n",
      "Dev loss in epoch [7/20]: 36.4186369417787\n",
      "Dev loss in epoch [8/20]: 35.78559679303716\n",
      "Dev loss in epoch [9/20]: 35.3607770874339\n",
      "Dev loss in epoch [10/20]: 34.86384987560665\n",
      "Dev loss in epoch [11/20]: 34.45483633877108\n",
      "Dev loss in epoch [12/20]: 34.29919359402812\n",
      "Dev loss in epoch [13/20]: 34.01566446814592\n",
      "Dev loss in epoch [14/20]: 33.7580281055873\n",
      "Dev loss in epoch [15/20]: 33.36938480828531\n",
      "Dev loss in epoch [16/20]: 33.3660480895525\n",
      "Dev loss in epoch [17/20]: 33.27893377521297\n",
      "Dev loss in epoch [18/20]: 33.106212687820076\n",
      "Dev loss in epoch [19/20]: 33.0229995989753\n",
      "Dev loss in epoch [20/20]: 32.989368349676994\n",
      "\n",
      "Average loss per epoch: 50.132882228050065\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1e-05:\n",
      "##############################\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss in epoch [1/20]: 387.3786317495285\n",
      "Dev loss in epoch [2/20]: 205.02590205323028\n",
      "Dev loss in epoch [3/20]: 99.91250709375095\n",
      "Dev loss in epoch [4/20]: 56.851562772430356\n",
      "Dev loss in epoch [5/20]: 42.386803345028866\n",
      "Dev loss in epoch [6/20]: 38.685497229838795\n",
      "Dev loss in epoch [7/20]: 37.03667226945825\n",
      "Dev loss in epoch [8/20]: 36.27340593819038\n",
      "Dev loss in epoch [9/20]: 35.82757644775987\n",
      "Dev loss in epoch [10/20]: 35.50451500231968\n",
      "Dev loss in epoch [11/20]: 35.2178020465175\n",
      "Dev loss in epoch [12/20]: 35.06376121420678\n",
      "Dev loss in epoch [13/20]: 34.71987904616878\n",
      "Dev loss in epoch [14/20]: 34.470403413275314\n",
      "Dev loss in epoch [15/20]: 34.2917297617234\n",
      "Dev loss in epoch [16/20]: 34.08648649265811\n",
      "Dev loss in epoch [17/20]: 33.89445012808373\n",
      "Dev loss in epoch [18/20]: 33.82911586801958\n",
      "Dev loss in epoch [19/20]: 33.75177412909629\n",
      "Dev loss in epoch [20/20]: 33.609118329286005\n",
      "\n",
      "Average loss per epoch: 65.89087971652856\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1e-05:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 65.02219352950293\n",
      "Dev loss in epoch [2/20]: 36.10116002478589\n",
      "Dev loss in epoch [3/20]: 33.772055411291504\n",
      "Dev loss in epoch [4/20]: 33.33806130235354\n",
      "Dev loss in epoch [5/20]: 32.96820677942051\n",
      "Dev loss in epoch [6/20]: 32.87392698159014\n",
      "Dev loss in epoch [7/20]: 32.89882699487705\n",
      "Dev loss in epoch [8/20]: 32.8153173955846\n",
      "Dev loss in epoch [9/20]: 33.11072879076107\n",
      "Dev loss in epoch [10/20]: 32.945761259495775\n",
      "Dev loss in epoch [11/20]: 32.73289663040228\n",
      "Dev loss in epoch [12/20]: 32.81775923735641\n",
      "Dev loss in epoch [13/20]: 32.99925026281068\n",
      "Dev loss in epoch [14/20]: 32.73881527373767\n",
      "Dev loss in epoch [15/20]: 32.71860178267757\n",
      "Dev loss in epoch [16/20]: 32.76898930970696\n",
      "Dev loss in epoch [17/20]: 32.891324487940054\n",
      "Dev loss in epoch [18/20]: 32.858920523314275\n",
      "Dev loss in epoch [19/20]: 32.84959502465711\n",
      "Dev loss in epoch [20/20]: 32.9339795634558\n",
      "\n",
      "Average loss per epoch: 34.70781852828609\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 0.0001:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 34.53408173895302\n",
      "Dev loss in epoch [2/20]: 35.87870150094708\n",
      "Dev loss in epoch [3/20]: 34.30822038113342\n",
      "Dev loss in epoch [4/20]: 33.18964774585163\n",
      "Dev loss in epoch [5/20]: 33.109819931870646\n",
      "Dev loss in epoch [6/20]: 33.22084479864344\n",
      "Dev loss in epoch [7/20]: 34.205490519499364\n",
      "Dev loss in epoch [8/20]: 33.04670196599502\n",
      "Dev loss in epoch [9/20]: 33.31271785805139\n",
      "Dev loss in epoch [10/20]: 33.91465236824393\n",
      "Dev loss in epoch [11/20]: 33.020285563259826\n",
      "Dev loss in epoch [12/20]: 34.878796317587586\n",
      "Dev loss in epoch [13/20]: 34.249054127247106\n",
      "Dev loss in epoch [14/20]: 32.940630948901166\n",
      "Dev loss in epoch [15/20]: 32.92510220222891\n",
      "Dev loss in epoch [16/20]: 35.21389089038484\n",
      "Dev loss in epoch [17/20]: 33.34767501410839\n",
      "Dev loss in epoch [18/20]: 33.941708814038805\n",
      "Dev loss in epoch [19/20]: 33.39873494915849\n",
      "Dev loss in epoch [20/20]: 32.89902088669251\n",
      "\n",
      "Average loss per epoch: 33.77678892613983\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 0.0001:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 34.100599885952505\n",
      "Dev loss in epoch [2/20]: 37.055935880547565\n",
      "Dev loss in epoch [3/20]: 33.226255615183526\n",
      "Dev loss in epoch [4/20]: 33.15209867325767\n",
      "Dev loss in epoch [5/20]: 33.062247504198524\n",
      "Dev loss in epoch [6/20]: 33.14471275182098\n",
      "Dev loss in epoch [7/20]: 33.3558620126287\n",
      "Dev loss in epoch [8/20]: 33.01274032592653\n",
      "Dev loss in epoch [9/20]: 34.00077275905643\n",
      "Dev loss in epoch [10/20]: 35.5792484149181\n",
      "Dev loss in epoch [11/20]: 34.2194181918376\n",
      "Dev loss in epoch [12/20]: 35.55173130148581\n",
      "Dev loss in epoch [13/20]: 34.45175556188425\n",
      "Dev loss in epoch [14/20]: 33.58448883340026\n",
      "Dev loss in epoch [15/20]: 33.79564636660973\n",
      "Dev loss in epoch [16/20]: 33.556468925554086\n",
      "Dev loss in epoch [17/20]: 32.91120657866841\n",
      "Dev loss in epoch [18/20]: 32.869447363028144\n",
      "Dev loss in epoch [19/20]: 33.998213037130824\n",
      "Dev loss in epoch [20/20]: 33.05007613029531\n",
      "\n",
      "Average loss per epoch: 33.88394630566925\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 0.0001:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 33.12448231312929\n",
      "Dev loss in epoch [2/20]: 35.49378846237294\n",
      "Dev loss in epoch [3/20]: 33.13041658997985\n",
      "Dev loss in epoch [4/20]: 32.90484350445853\n",
      "Dev loss in epoch [5/20]: 33.62611154838579\n",
      "Dev loss in epoch [6/20]: 33.657491844618214\n",
      "Dev loss in epoch [7/20]: 32.83166426314477\n",
      "Dev loss in epoch [8/20]: 32.863483849079024\n",
      "Dev loss in epoch [9/20]: 39.23467762114623\n",
      "Dev loss in epoch [10/20]: 32.74800226290275\n",
      "Dev loss in epoch [11/20]: 33.19164747753881\n",
      "Dev loss in epoch [12/20]: 36.01148079222353\n",
      "Dev loss in epoch [13/20]: 35.836028727146946\n",
      "Dev loss in epoch [14/20]: 32.869312748492135\n",
      "Dev loss in epoch [15/20]: 32.79544215738494\n",
      "Dev loss in epoch [16/20]: 44.683187943693724\n",
      "Dev loss in epoch [17/20]: 34.44650176333969\n",
      "Dev loss in epoch [18/20]: 32.79632244484334\n",
      "Dev loss in epoch [19/20]: 34.38433837857056\n",
      "Dev loss in epoch [20/20]: 32.99018147955718\n",
      "\n",
      "Average loss per epoch: 34.48097030860041\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 0.001:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 1377.6267246520845\n",
      "Dev loss in epoch [2/20]: 100.7767146492206\n",
      "Dev loss in epoch [3/20]: 528.8570551106985\n",
      "Dev loss in epoch [4/20]: 518.4036952248449\n",
      "Dev loss in epoch [5/20]: 495.90893055944593\n",
      "Dev loss in epoch [6/20]: 460.3773574963083\n",
      "Dev loss in epoch [7/20]: 427.0492338361126\n",
      "Dev loss in epoch [8/20]: 407.21995646969657\n",
      "Dev loss in epoch [9/20]: 377.8463360892566\n",
      "Dev loss in epoch [10/20]: 350.8544616526329\n",
      "Dev loss in epoch [11/20]: 326.19411876843003\n",
      "Dev loss in epoch [12/20]: 303.1849601920687\n",
      "Dev loss in epoch [13/20]: 282.0962893974711\n",
      "Dev loss in epoch [14/20]: 262.45563891814817\n",
      "Dev loss in epoch [15/20]: 244.11373934880413\n",
      "Dev loss in epoch [16/20]: 227.23208542881846\n",
      "Dev loss in epoch [17/20]: 211.55950348931063\n",
      "Dev loss in epoch [18/20]: 197.11653972149034\n",
      "Dev loss in epoch [19/20]: 183.71500544440465\n",
      "Dev loss in epoch [20/20]: 171.08140491506896\n",
      "\n",
      "Average loss per epoch: 372.68348756821575\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 0.001:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 50.25273751764023\n",
      "Dev loss in epoch [2/20]: 596.2255910053765\n",
      "Dev loss in epoch [3/20]: 566.0064908277037\n",
      "Dev loss in epoch [4/20]: 532.7094927303804\n",
      "Dev loss in epoch [5/20]: 493.63297264142614\n",
      "Dev loss in epoch [6/20]: 463.54281801363805\n",
      "Dev loss in epoch [7/20]: 430.4005171540257\n",
      "Dev loss in epoch [8/20]: 399.8786060631323\n",
      "Dev loss in epoch [9/20]: 371.57289119015724\n",
      "Dev loss in epoch [10/20]: 345.1842717756003\n",
      "Dev loss in epoch [11/20]: 320.4980235854348\n",
      "Dev loss in epoch [12/20]: 297.9753848715101\n",
      "Dev loss in epoch [13/20]: 277.0485840637739\n",
      "Dev loss in epoch [14/20]: 257.76687315234295\n",
      "Dev loss in epoch [15/20]: 239.74398053270798\n",
      "Dev loss in epoch [16/20]: 223.17628376720378\n",
      "Dev loss in epoch [17/20]: 207.82653177873607\n",
      "Dev loss in epoch [18/20]: 193.5928487436199\n",
      "Dev loss in epoch [19/20]: 180.32579604612806\n",
      "Dev loss in epoch [20/20]: 168.1090877665801\n",
      "\n",
      "Average loss per epoch: 330.7734891613559\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 0.001:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 614.4816816337886\n",
      "Dev loss in epoch [2/20]: 576.8945870790393\n",
      "Dev loss in epoch [3/20]: 535.4918864492856\n",
      "Dev loss in epoch [4/20]: 496.9307300190962\n",
      "Dev loss in epoch [5/20]: 461.3173240731463\n",
      "Dev loss in epoch [6/20]: 428.3641813223124\n",
      "Dev loss in epoch [7/20]: 397.7226507613578\n",
      "Dev loss in epoch [8/20]: 369.4311704468654\n",
      "Dev loss in epoch [9/20]: 343.2544284120812\n",
      "Dev loss in epoch [10/20]: 319.01443161740457\n",
      "Dev loss in epoch [11/20]: 296.5110676769604\n",
      "Dev loss in epoch [12/20]: 275.6628566620796\n",
      "Dev loss in epoch [13/20]: 256.33343905258283\n",
      "Dev loss in epoch [14/20]: 238.55605637595846\n",
      "Dev loss in epoch [15/20]: 222.07499683462683\n",
      "Dev loss in epoch [16/20]: 206.67360675358634\n",
      "Dev loss in epoch [17/20]: 192.38024187754945\n",
      "Dev loss in epoch [18/20]: 179.24670415419598\n",
      "Dev loss in epoch [19/20]: 167.12952383418985\n",
      "Dev loss in epoch [20/20]: 155.90619265156755\n",
      "\n",
      "Average loss per epoch: 336.6688878843837\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 0.01:\n",
      "##############################\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss in epoch [1/20]: 4337.205858756585\n",
      "Dev loss in epoch [2/20]: 2012.0023858422805\n",
      "Dev loss in epoch [3/20]: 936.5848961213869\n",
      "Dev loss in epoch [4/20]: 439.2619760370247\n",
      "Dev loss in epoch [5/20]: 209.21596351560626\n",
      "Dev loss in epoch [6/20]: 105.50113286504892\n",
      "Dev loss in epoch [7/20]: 56.87332111820653\n",
      "Dev loss in epoch [8/20]: 34.450911708667206\n",
      "Dev loss in epoch [9/20]: 24.81616070835396\n",
      "Dev loss in epoch [10/20]: 20.355718382845996\n",
      "Dev loss in epoch [11/20]: 18.48799260417882\n",
      "Dev loss in epoch [12/20]: 17.668285756742886\n",
      "Dev loss in epoch [13/20]: 17.272329922717855\n",
      "Dev loss in epoch [14/20]: 17.0962193742226\n",
      "Dev loss in epoch [15/20]: 17.062049213973832\n",
      "Dev loss in epoch [16/20]: 17.065396717849683\n",
      "Dev loss in epoch [17/20]: 17.079686209950466\n",
      "Dev loss in epoch [18/20]: 17.102757477097743\n",
      "Dev loss in epoch [19/20]: 17.11502343865052\n",
      "Dev loss in epoch [20/20]: 17.16159538271133\n",
      "\n",
      "Average loss per epoch: 417.4689830577051\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 0.01:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 8705.317834313557\n",
      "Dev loss in epoch [2/20]: 4032.039942854312\n",
      "Dev loss in epoch [3/20]: 1878.422924661627\n",
      "Dev loss in epoch [4/20]: 874.7809026957266\n",
      "Dev loss in epoch [5/20]: 412.7056070580724\n",
      "Dev loss in epoch [6/20]: 198.28497889169734\n",
      "Dev loss in epoch [7/20]: 99.04058582798838\n",
      "Dev loss in epoch [8/20]: 54.473613873047434\n",
      "Dev loss in epoch [9/20]: 33.88833273392599\n",
      "Dev loss in epoch [10/20]: 24.452836386835965\n",
      "Dev loss in epoch [11/20]: 19.87057337425283\n",
      "Dev loss in epoch [12/20]: 18.20337379169348\n",
      "Dev loss in epoch [13/20]: 17.51666631042812\n",
      "Dev loss in epoch [14/20]: 17.193242300415744\n",
      "Dev loss in epoch [15/20]: 17.07826895911096\n",
      "Dev loss in epoch [16/20]: 17.058014424100236\n",
      "Dev loss in epoch [17/20]: 17.058502260051963\n",
      "Dev loss in epoch [18/20]: 17.073433760879208\n",
      "Dev loss in epoch [19/20]: 17.122955021141998\n",
      "Dev loss in epoch [20/20]: 17.170567750808132\n",
      "\n",
      "Average loss per epoch: 824.4376578624837\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 0.01:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 4618.949592071259\n",
      "Dev loss in epoch [2/20]: 2147.8623678670583\n",
      "Dev loss in epoch [3/20]: 1001.2614252179208\n",
      "Dev loss in epoch [4/20]: 468.51717438358884\n",
      "Dev loss in epoch [5/20]: 222.75292609495787\n",
      "Dev loss in epoch [6/20]: 109.34066471203414\n",
      "Dev loss in epoch [7/20]: 58.542898824185535\n",
      "Dev loss in epoch [8/20]: 35.88451602999787\n",
      "Dev loss in epoch [9/20]: 24.71411667076087\n",
      "Dev loss in epoch [10/20]: 20.392581208308105\n",
      "Dev loss in epoch [11/20]: 18.468940669949692\n",
      "Dev loss in epoch [12/20]: 17.623175635095475\n",
      "Dev loss in epoch [13/20]: 17.188581754806343\n",
      "Dev loss in epoch [14/20]: 17.06231680391105\n",
      "Dev loss in epoch [15/20]: 17.06109268830989\n",
      "Dev loss in epoch [16/20]: 17.097730084553604\n",
      "Dev loss in epoch [17/20]: 17.13088391756044\n",
      "Dev loss in epoch [18/20]: 17.130514605254575\n",
      "Dev loss in epoch [19/20]: 17.160701829166545\n",
      "Dev loss in epoch [20/20]: 17.144685293645654\n",
      "\n",
      "Average loss per epoch: 444.0643443181161\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 0.1:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 75509162076.49062\n",
      "Dev loss in epoch [2/20]: 15671671.156177746\n",
      "Dev loss in epoch [3/20]: 3289.6378779835336\n",
      "Dev loss in epoch [4/20]: 17.071527110037223\n",
      "Dev loss in epoch [5/20]: 17.39144871829743\n",
      "Dev loss in epoch [6/20]: 17.112574092458587\n",
      "Dev loss in epoch [7/20]: 17.62593177766422\n",
      "Dev loss in epoch [8/20]: 18.619462420267663\n",
      "Dev loss in epoch [9/20]: 17.164205485924125\n",
      "Dev loss in epoch [10/20]: 17.221888568891117\n",
      "Dev loss in epoch [11/20]: 17.308114746086805\n",
      "Dev loss in epoch [12/20]: 17.11769408949686\n",
      "Dev loss in epoch [13/20]: 17.077680184792566\n",
      "Dev loss in epoch [14/20]: 17.195186203451573\n",
      "Dev loss in epoch [15/20]: 17.220271070516496\n",
      "Dev loss in epoch [16/20]: 17.25937356389605\n",
      "Dev loss in epoch [17/20]: 17.067770413645125\n",
      "Dev loss in epoch [18/20]: 17.28794741537066\n",
      "Dev loss in epoch [19/20]: 17.05784947854712\n",
      "Dev loss in epoch [20/20]: 17.061943100285944\n",
      "\n",
      "Average loss per epoch: 3776241866.5572777\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 0.1:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 1.07423486389148e+16\n",
      "Dev loss in epoch [2/20]: 2231099825360.222\n",
      "Dev loss in epoch [3/20]: 463360872.9957087\n",
      "Dev loss in epoch [4/20]: 96415.10793680647\n",
      "Dev loss in epoch [5/20]: 35.88744655468468\n",
      "Dev loss in epoch [6/20]: 17.060518950685836\n",
      "Dev loss in epoch [7/20]: 17.08093946937878\n",
      "Dev loss in epoch [8/20]: 17.22480010533015\n",
      "Dev loss in epoch [9/20]: 17.072087992296805\n",
      "Dev loss in epoch [10/20]: 17.16855446050207\n",
      "Dev loss in epoch [11/20]: 17.089486893810225\n",
      "Dev loss in epoch [12/20]: 17.064211478332005\n",
      "Dev loss in epoch [13/20]: 17.462270176502773\n",
      "Dev loss in epoch [14/20]: 17.057980193604063\n",
      "Dev loss in epoch [15/20]: 17.243863825507088\n",
      "Dev loss in epoch [16/20]: 17.12013481466807\n",
      "Dev loss in epoch [17/20]: 17.389018256113623\n",
      "Dev loss in epoch [18/20]: 17.123235219266192\n",
      "Dev loss in epoch [19/20]: 17.791714476940935\n",
      "Dev loss in epoch [20/20]: 18.20060135380417\n",
      "\n",
      "Average loss per epoch: 537229010109887.7\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 0.1:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 185870.5752264976\n",
      "Dev loss in epoch [2/20]: 52.26755087687658\n",
      "Dev loss in epoch [3/20]: 17.333644050984457\n",
      "Dev loss in epoch [4/20]: 17.085917093394116\n",
      "Dev loss in epoch [5/20]: 17.288098347352\n",
      "Dev loss in epoch [6/20]: 17.314497316169803\n",
      "Dev loss in epoch [7/20]: 17.173747287775807\n",
      "Dev loss in epoch [8/20]: 17.913809461361474\n",
      "Dev loss in epoch [9/20]: 17.18717816049215\n",
      "Dev loss in epoch [10/20]: 17.15906406634037\n",
      "Dev loss in epoch [11/20]: 17.072319357700195\n",
      "Dev loss in epoch [12/20]: 17.07618057559794\n",
      "Dev loss in epoch [13/20]: 17.522815816081593\n",
      "Dev loss in epoch [14/20]: 17.103598160040725\n",
      "Dev loss in epoch [15/20]: 17.087247622883993\n",
      "Dev loss in epoch [16/20]: 17.630312884475135\n",
      "Dev loss in epoch [17/20]: 17.06825935385781\n",
      "Dev loss in epoch [18/20]: 17.083514236001708\n",
      "Dev loss in epoch [19/20]: 17.06189064449289\n",
      "Dev loss in epoch [20/20]: 17.059398051475654\n",
      "\n",
      "Average loss per epoch: 9311.65321349305\n",
      "\n",
      "##############################\n",
      "[1/0]Training model for learning rate of 1:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 1.1185049794890792e+33\n",
      "Dev loss in epoch [2/20]: 1.1185049794890756e+33\n",
      "Dev loss in epoch [3/20]: 1.1185049794890731e+33\n",
      "Dev loss in epoch [4/20]: 1.1185049794890756e+33\n",
      "Dev loss in epoch [5/20]: 1.1185049794890773e+33\n",
      "Dev loss in epoch [6/20]: 1.1185049794890792e+33\n",
      "Dev loss in epoch [7/20]: 1.1185049794890806e+33\n",
      "Dev loss in epoch [8/20]: 1.11850497948908e+33\n",
      "Dev loss in epoch [9/20]: 1.1185049794890792e+33\n",
      "Dev loss in epoch [10/20]: 1.118504979489082e+33\n",
      "Dev loss in epoch [11/20]: 1.1185049794890809e+33\n",
      "Dev loss in epoch [12/20]: 1.118504979489082e+33\n",
      "Dev loss in epoch [13/20]: 1.11850497948908e+33\n",
      "Dev loss in epoch [14/20]: 1.118504979489082e+33\n",
      "Dev loss in epoch [15/20]: 1.11850497948908e+33\n",
      "Dev loss in epoch [16/20]: 1.1185049794890812e+33\n",
      "Dev loss in epoch [17/20]: 1.11850497948908e+33\n",
      "Dev loss in epoch [18/20]: 1.11850497948908e+33\n",
      "Dev loss in epoch [19/20]: 1.1185049794890818e+33\n",
      "Dev loss in epoch [20/20]: 1.118504979489082e+33\n",
      "\n",
      "Average loss per epoch: 1.1185049794890795e+33\n",
      "\n",
      "##############################\n",
      "[2/1]Training model for learning rate of 1:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 1411216840883952.8\n",
      "Dev loss in epoch [2/20]: 1411216166502782.5\n",
      "Dev loss in epoch [3/20]: 1411216714971564.2\n",
      "Dev loss in epoch [4/20]: 1411217558404383.8\n",
      "Dev loss in epoch [5/20]: 1411217214254108.0\n",
      "Dev loss in epoch [6/20]: 1411217365426944.5\n",
      "Dev loss in epoch [7/20]: 1411216605222015.8\n",
      "Dev loss in epoch [8/20]: 1411217226000098.0\n",
      "Dev loss in epoch [9/20]: 1411218385041640.0\n",
      "Dev loss in epoch [10/20]: 1411217115489811.8\n",
      "Dev loss in epoch [11/20]: 1411217096117891.2\n",
      "Dev loss in epoch [12/20]: 1411216139999858.5\n",
      "Dev loss in epoch [13/20]: 1411216830168204.8\n",
      "Dev loss in epoch [14/20]: 1411216657886940.2\n",
      "Dev loss in epoch [15/20]: 1411217065933474.5\n",
      "Dev loss in epoch [16/20]: 1411216761193896.8\n",
      "Dev loss in epoch [17/20]: 1411216107564312.5\n",
      "Dev loss in epoch [18/20]: 1411216480198998.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss in epoch [19/20]: 1411216624540415.5\n",
      "Dev loss in epoch [20/20]: 1411216528302467.0\n",
      "\n",
      "Average loss per epoch: 1411216874205188.2\n",
      "\n",
      "##############################\n",
      "[3/2]Training model for learning rate of 1:\n",
      "##############################\n",
      "\n",
      "Dev loss in epoch [1/20]: 198112206502614.44\n",
      "Dev loss in epoch [2/20]: 198111809266985.88\n",
      "Dev loss in epoch [3/20]: 198111815228901.78\n",
      "Dev loss in epoch [4/20]: 198111236828214.44\n",
      "Dev loss in epoch [5/20]: 198111351095278.62\n",
      "Dev loss in epoch [6/20]: 198110832986319.28\n",
      "Dev loss in epoch [7/20]: 198111182192873.38\n",
      "Dev loss in epoch [8/20]: 198111434243490.72\n",
      "Dev loss in epoch [9/20]: 198111620361048.75\n",
      "Dev loss in epoch [10/20]: 198111664675798.62\n",
      "Dev loss in epoch [11/20]: 198112160328812.22\n",
      "Dev loss in epoch [12/20]: 198113024445903.28\n",
      "Dev loss in epoch [13/20]: 198113047428349.97\n",
      "Dev loss in epoch [14/20]: 198112549008441.16\n",
      "Dev loss in epoch [15/20]: 198112491941421.1\n",
      "Dev loss in epoch [16/20]: 198112182285115.16\n",
      "Dev loss in epoch [17/20]: 198112209708248.75\n",
      "Dev loss in epoch [18/20]: 198112776833956.47\n",
      "Dev loss in epoch [19/20]: 198112562247270.12\n",
      "Dev loss in epoch [20/20]: 198112320708016.75\n",
      "\n",
      "Average loss per epoch: 198112023915853.0\n"
     ]
    }
   ],
   "source": [
    "x_points = list()\n",
    "y_points = list()\n",
    "for i in range(-10, 1, 1):\n",
    "    x_points.append(i)\n",
    "    y_three_points = list()\n",
    "    for j in range(3):\n",
    "        print(\"\")\n",
    "        print(f\"##############################\")\n",
    "        print(f\"[{j+1}/{j}]Training model for learning rate of {10**i}:\")\n",
    "        print(f\"##############################\")\n",
    "        print(\"\")\n",
    "        network = Assignment_5_Network(10**i)\n",
    "        y_three_points.append(network.optimize(train_dataloader, dev_dataloader, 20))\n",
    "    y_points.append(sum(y_three_points) / len(y_three_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa86e94ce80>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Avg MSE loss per epoch, averaged over 3 runs\")\n",
    "plt.ylabel(\"Avg MSE Loss\")\n",
    "plt.xlabel(\"Learning rate 1e-x\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(x_points, y_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypotheses:\n",
    "\n",
    "The graph tells us that the average loss per epoch is about the same for learning rates from $10^{-10}$ to $10^{-2}$, which lies somewhere between $\\approx100$ - $\\approx1000$. For learning rates from $10^{-1}$ and $10^{0}$, the average MSE loss is many orders of magnitude higher. This could be due to the fact that for lower learning rates, the model approaches a local minimum bit by bit, while it makes huge jumps for higher learning rates. It's not even clear whether the high learning rates result in any improvement, or whether the model is just bouncing around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
