{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwL-VcGY6shN"
   },
   "source": [
    "# NNIA Assignment 6\n",
    "**DEADLINE: 15.12.2023 0800 CET**\n",
    "- Name & ID 1 (CMS username):\n",
    "- Name & ID 2 (CMS username):\n",
    "- Hours of work per person:\n",
    "\n",
    "# Submission Instructions\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the submission deadline. All course-related questions can be addressed on the course **CMS Forum**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2-3**. It is fine to submit first **2** assignments without a team, but starting from the **3rd** assignment it is not allowed.\n",
    "* Please include your **names**, **ID's**, **CMS usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (`.zip` is the only accepted extension) in **CMS**.\n",
    "* Only **one** member of the group should make the submission.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2(_Name3_id3).zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization repeatedly students fail to do this.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Partial Derivatives of Vector-valued Functions: Jacobian and Hessian Matrices (3 points)\n",
    "\n",
    "### 1.1\n",
    "Compute the *Jacobian* and *Hessian* Matrices of the following function below. Include the generalized formula for the calculations. (Hint: don't forget the chain rule) (2 points)\n",
    "\n",
    "\n",
    "\\begin{align*}f(x,y) &= \\begin{bmatrix} f_1(x,y): 5x^2 y^2 + 2x^5 y^3 - 8x^4y^3 \\\\ f_2(x,y): x^3y^2 + 3x^2y + x \\cdot e^{2y}\\\\  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### 1.2\n",
    "In general, describe the characteristics of a Hessian Matrix and include any specific names (if any) of the matrix for a function $f$ at some point $x$ where $x$ is:  (1 point)\n",
    "\n",
    "1.   a minimum,\n",
    "2.   a maximum,\n",
    "3.   a saddle point\n",
    "4.   Undeterminable without more information or visualization\n",
    "\n",
    "(Note: the function $f$ here does not refer to the function in 1.1, but rather some function in general.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 Activation Functions (2 points)\n",
    "\n",
    "Three of the most commonly-used activation functions are the sigmoid function, hyperbolic tangent, and ReLU. The equations for these functions are provided below:\n",
    "\n",
    "* $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "* $\\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}$\n",
    "\n",
    "* $\\text{ReLU}(x) =  \\bigg\\{ \\begin{array}{ll} 0, & x < 0 \\\\ x, & x > 0 \\\\ \\end{array}$\n",
    "\n",
    "Again, using code or an online resource like Wolfram Alpha or Desmos, graph each function along with its derivative. Make sure to include the plots in your solution.\n",
    "1. Discuss the differences you observe. (1 sentence)\n",
    "2. What are the advantages and disadvantages of each? In particular, think about how the range of the function and the amplitude of the derivative would affect a network. (2 sentences)\n",
    "3. Which one of the activation functions would be most appropriate for a binary classification problem?  Would adding more classes change your choice? Why or\n",
    "why not? (2-3 sentences)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2 Batch Normalization (5 points)\n",
    "In this exercise you will again construct a feed forward neural network, however you will use batch normalization during training. Training deep neural networks can be challenging due to the change in the distribution of inputs to layers deep in the network as a result of the updates of the weights in the previous layers. This causes the learning to chase a \"moving target\", which slows down the learning process. Batch normalization is a technique that aims to address this problem by normalizing layer inputs. This stabilizes the learning process and can greatly decrease training time. If you are interested you can read the paper introducing batch normalization [here](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "You will be working with the [FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html) dataset by Zalando, which consists of $28\\times 28$ black and white images and has 10 classes just like the [MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) dataset. But instead of numbers the classes are various items of clothing such as shoes, t-shirts, dresses, etc.\n",
    "\n",
    "\n",
    "## 2.1 Baseline Network (2 points)\n",
    "You are provided with a dataloader for the train and test set, each with a batch size of 64. You're task is to construct a feed forward neural network with 3 hidden linear layers with ReLU after the first 2 layers. The first layer should have a hidden size of 64 and the second a hidden size of 32. This is a multi-class classification problem so you will need to use cross entropy loss (provided by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)). Use a stochastic gradient descent with a learning rate of 0.001 as the optimizer. Train your network on the training data for 5 epochs and report accuracy on the training set after each epoch. Make sure to save the accuracies.\n",
    "(hint: the data comes in the format of $28 \\times 28$ tensors, so you will need flatten it to train your network)\n",
    "\n",
    "## 2.2 Network with Batch Normalization (2 points)\n",
    "Construct another network with the same parameters as 2.1 but this time include a batch normalization layer (use [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)). Where to place this layer is up to you, but you can reference the lecture slides for inspiration. Again train your network for 5 epochs and report test accuracy after each epoch.\n",
    "\n",
    "## 2.3 Plotting the Performances (1 point)\n",
    "Plot the accuracies of the 2 networks and discus the differences you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-12-05T14:49:38.767655500Z",
     "start_time": "2023-12-05T14:49:38.741820700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-12-05T14:49:39.473422Z",
     "start_time": "2023-12-05T14:49:39.361915800Z"
    }
   },
   "outputs": [],
   "source": [
    "# load train and test set\n",
    "fashion_trainset = torchvision.datasets.FashionMNIST('data/', train=True, download=True, transform=transforms.ToTensor())\n",
    "fashion_testset = torchvision.datasets.FashionMNIST('data/', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2023-12-05T14:49:40.181659Z",
     "start_time": "2023-12-05T14:49:40.145005400Z"
    }
   },
   "outputs": [],
   "source": [
    "# get train and test loader\n",
    "fashion_train_loader = torch.utils.data.DataLoader(dataset=fashion_trainset, batch_size=64, shuffle=True)\n",
    "fashion_test_loader = torch.utils.data.DataLoader(dataset=fashion_testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus (1 point)\n",
    "Try placing the normalization layer in different places in the network and include your results in the plot from 2.3. What do you observe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
