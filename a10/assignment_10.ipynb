{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwL-VcGY6shN"
   },
   "source": [
    "# NNIA Assignment 10\n",
    "**DEADLINE: 02.02.2024 0800 CET**\n",
    "- Name & ID 1 (CMS username):\n",
    "- Name & ID 2 (CMS username):\n",
    "- Hours of work per person:\n",
    "\n",
    "# Submission Instructions\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the submission deadline. All course-related questions can be addressed on the course **CMS Forum**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2-3**. It is fine to submit first **2** assignments without a team, but starting from the **3rd** assignment it is not allowed.\n",
    "* Please include your **names**, **ID's**, **CMS usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (`.zip` is the only accepted extension) in **CMS**.\n",
    "* Only **one** member of the group should make the submission.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2(_Name3_id3).zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization repeatedly students fail to do this.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 RNNs and LSTMs (2 points)\n",
    "In a fully connected neural network the number of weight matrices to learn during the training depends on the number of hidden layers, and the size of the matrices depends on the number of neurons (for hidden layers), the number of parameters (for the input layer) and the size of the output (for the output layer).\n",
    "\n",
    "When we are dealing with sequential data (e.g. language) our input is not a single datapoint with a certain number of features, but a sequence of inputs (e.g words).\n",
    "\n",
    "1. Does the number of weight matrices in an RNN depend on the length of the sequence? Justify your answer by drawing an unfolded graph of a simple RNN and supporting it with a short answer (1-2 sentences).\n",
    "2. Explain what problem the LSTM type of recurrent network solves and what each of the gates it uses addresses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 <font color=\"red\">To Do</font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Training an RNN on the IMDB dataset (5 points)\n",
    "\n",
    "In this exercise you will be training an RNN on the IMDB dataset, where the task is to predict if a movie review is positive or negative i.e. performing sentiment analysis. The PyTorch version of the dataset can be found [here](https://pytorch.org/text/stable/datasets.html#imdb). Alternatively, the dataset is also available on Huggingface [here](https://huggingface.co/datasets/imdb) (note: you will need to use the huggingface version in exercise 3).\n",
    "### 2.1 Pre-Processing and building a Dataloader\n",
    "Neural networks can't handle raw text so you will need to tokenize the words and find some vector representation for them. Some options for this include [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings, [fastText](https://fasttext.cc/docs/en/crawl-vectors.html) embeddings, or alternatively you can just train your own using [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) or simply use [one-hot-encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/). Depending on how you define your network you will also need to take care of padding as the reviews are of different lengths. And depending on how you choose to represent words you will need to handle unknown words (out-of-vocabulary words). The simplest strategy would be to ignore unknown words, but other strategies may yield better results.\n",
    "Build a PyTorch dataloader for the training and test set. You may want to consider creating a validation set as well.\n",
    "Notes:\n",
    "   - We also recommend that you start working with a smaller subset of the data as this allows for easier debugging\n",
    "   - PyTorch provides some useful modules for this task such as:\n",
    "        - [torchtext.vocab](https://pytorch.org/text/stable/vocab.html) for indexing the vocabulary of the dataset.\n",
    "        - [torch.nn.utils.rnn.pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) for padding.\n",
    "   - Also note that in the PyTorch version of the dataset labels for negative and positive are 1 and 2, so you will need to convert them to 0 (negative) and 1 (positive).\n",
    "\n",
    "\n",
    "### 2.2 Building and Training an RNN\n",
    "1. Next your task is to build an RNN followed by a classifier. Also feel free to use non-vanilla RNN cells (such as LSTM or GRU).\n",
    "2. Train your network for at least 5 epochs. Report train loss after every epoch and train and test accuracy when you are done training. Feel free to report train and validation (if you have a validation set) accuracy as well as you train.\n",
    "\n",
    "\n",
    "Notes:\n",
    "- Besides using an RNN, the network structure and hyperparameters are up to you but try to choose them sensibly.\n",
    "- You may want to consider using a gpu for this task (i.e. the SIC-cluster). Alternatively you could use Google Colab.\n",
    "- The following guides may also be helpful for this task:\n",
    "    - [Pytorch [Basics] â€” Intro to RNN](https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677)\n",
    "    - [SEQUENCE MODELS AND LONG SHORT-TERM MEMORY NETWORKS](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "    - [Using LSTM in PyTorch: A Tutorial With Examples](https://wandb.ai/sauravmaheshkar/LSTM-PyTorch/reports/Using-LSTM-in-PyTorch-A-Tutorial-With-Examples--VmlldzoxMDA2NTA5)\n",
    "    - [How to apply LSTM using PyTorch](https://cnvrg.io/pytorch-lstm/)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 HuggingFace and Pre-trained models (3 points)\n",
    "Now that you have trained an RNN on sentiment analysis, you will be fine-tuning a pre-trained transformer on the same dataset using [HuggingFace](https://huggingface.co/) (HF).\n",
    "### 3.1 Introduction to fine-tuning with HuggingFace (1 point)\n",
    "Read the [chapter 3](https://huggingface.co/learn/nlp-course/chapter3/1?fw=pt) of the HuggingFace NLP Course and answer the questions in the end of chapter quiz.\n",
    "\n",
    "**Write your answers here**:\n",
    "\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "6.\n",
    "7.\n",
    "8.\n",
    "9.\n",
    "\n",
    "\n",
    "### 3.2 Fine-tuning Roberta on the IMDB dataset (2 points)\n",
    "Based on what you learned in 3.1 fine-tune [RoBERTa](https://huggingface.co/roberta-base) or [DistilRoBERTa](https://huggingface.co/distilroberta-base) (if would rather use a smaller model) on the [IMDB dataset](https://huggingface.co/datasets/imdb). Download the dataset, create an instance the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class, train the model and evaluate on the test set (compute and report accuracy).\n",
    "\n",
    "Notes:\n",
    "- This task may require a gpu, so again the options are the SIC-cluster or Google Colab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
