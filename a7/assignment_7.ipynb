{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwL-VcGY6shN"
   },
   "source": [
    "# NNIA Assignment 7\n",
    "\n",
    "## <font color=\"red\">Happy</font> <font color=\"green\">Holidays</font>!!\n",
    "\n",
    "**DEADLINE: 05.01.2024 0800 CET**\n",
    "- Name & ID 1 (CMS username):\n",
    "- Name & ID 2 (CMS username):\n",
    "- Hours of work per person:\n",
    "\n",
    "# Submission Instructions\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the submission deadline. All course-related questions can be addressed on the course **CMS Forum**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2-3**. It is fine to submit first **2** assignments without a team, but starting from the **3rd** assignment it is not allowed.\n",
    "* Please include your **names**, **ID's**, **CMS usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (`.zip` is the only accepted extension) in **CMS**.\n",
    "* Only **one** member of the group should make the submission.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2(_Name3_id3).zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization repeatedly students fail to do this.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 The Chain Rule in Computational Graphs (2 pts)\n",
    "\n",
    "The structure of neural networks is often represented using computational graphs to make complex operations easier to understand. If you are unfamiliar with computational graphs, read this [Intro to Computational Graphs in Deep Learning](https://www..org/computational-graphs-in-deep-learning/) and/or watch these videos on [Computational Graphs](https://youtu.be/hCP1vGoCdYU) and [Derivatives on Compuataional Graphs](https://youtu.be/nJyUyKN-XBQ) by DeeplearningAI before attempting this exercise.  \n",
    "\n",
    "Below is an example of a simple computation graph. Using this, write down the expressions (by applying the chain rule) and calculate the final values for the following partial derivatives:\n",
    "\n",
    "1.   $\\frac{\\partial e}{\\partial b}$\n",
    "2.   $\\frac{\\partial e}{\\partial a}$\n",
    "\n",
    "<img src=\"computgraph.png\" alt=\"Computional Graph\" width=641/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 <font color=\"red\">To Do</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Neural Network Implementation: Training, Forward Pass, and Backpropagation  (8 pts)\n",
    "\n",
    "At this point you already got familiar with the basic architecture of neural networks: hidden layers, activation\n",
    "functions, loss functions, forward pass and back-propagation.\n",
    "\n",
    "It's time to put it all together!\n",
    "\n",
    "We have been playing around with PyTorch for a while. Although it is nice and convenient, we want to have a deeper understanding of what is really going on under the hood. For this purpose, we will create and train a simple 2-layer neural network from scratch using simple matrix operations and what we have learned so far.\n",
    "\n",
    "In this exercise, we will work with [the PyTorch Datasets Class](https://pytorch.org/vision/stable/datasets.html) to obtain\n",
    "[the CIFAR10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Our goal is to benchmark our simple neural network by classifying the images in this dataset into their proper classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 The CIFAR10 dataset (2 pts)\n",
    "\n",
    "#### 2.1.1 Getting to know the Dataset (0.5 pts)\n",
    "Have a look at [the CIFAR10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html) and answer the following\n",
    "questions:\n",
    "\n",
    "1. What is the size of the dataset? Is it already divided into train and test sets?\n",
    "2. What is the input? What shape does it have?\n",
    "3. What shape would the output have?\n",
    "4. What classes are represented in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1.1 <font color=\"red\"> Answer </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1.2 Pre-Processing  (0.5 pts)\n",
    "\n",
    "Let's load the dataset into our workspace. We will do that by creating an instance of class [`torchvision.datasets.CIFAR10`](https://pytorch.org/vision/stable/datasets.html#cifar). Have a look at the parameters of the class:\n",
    "- `root`\n",
    "- `train`\n",
    "- `download`\n",
    "- `transform`\n",
    "- `target_transform`\n",
    "\n",
    "Make sure you understand what each of them means.\n",
    "\n",
    "The input data that we load with torchvision are PIL images. We need to do some preprocessing before we can use the data for training. We can do that by passing an argument to the parameter `transform`.\n",
    "\n",
    "1. We need to transform PIL images to tensors. **TODO**: find the corresponding function in `torchvision.transforms`.\n",
    "2. It is a good practice to normalize your data before feeding it into the network. **TODO**: find the corresponding function in `torchvision.transforms`.\n",
    "3. Right now each image has 3 dimensions: number of rows, number of columns and number of color channels (the latter is 3 in our case, as those are RGB images. If interested, you can read more about this in [Image Processing in Python Numpy](https://www.pythoninformer.com/python-libraries/numpy/numpy-and-images/)). We want to reshape each datapoint to be a flattened vector with $\\text{size} = n_{row} \\cdot n_{col} \\cdot n_{channels}$. **TODO**: create a class that performs this function. \n",
    "\n",
    "#### 2.1.3 (1 pt)\n",
    "Now we have our train and test sets.\n",
    "However, we would like to be able to train different models with different hyperparameters in the future and compare them using a validation set (we will use the hold-out method).\n",
    "For that we need to split our train data.\n",
    "Let's use 10% of the data for validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Tasks 1.2 - 1.3, Please **complete the functions** in `solution.py` and run the following cells. Do **NOT** change the code cells below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Only run this cell after having completed your code in solution.py or you will get an error!\n",
    "from solution import get_cifar10_dataset\n",
    "\n",
    "train_loader, test_loader, val_loader, classes = get_cifar10_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Verify the data are as expected\n",
    "\n",
    "The follow code cells are provided for your convenience. Do **NOT** change"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print('images.shape:', images.shape)\n",
    "print('labels.shape:', labels.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in order to visualize the image, we have to reshape the Tensor back to the $C,H,W$ format.\n",
    "Because of our manipulation, the image sample is not so clear. However, it's still possible to make out the correct class label for the sample.\n",
    "(Hint: You should see an image of a deer corresponding to the label the *first* time you run this cell.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(23)\n",
    "idx = np.random.randint(0, 128, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(images[idx].reshape((3, 32, 32)).permute((1,2,0)).numpy())\n",
    "print('label:', classes[labels[idx]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see an image of an airplane below and its label the *first* time you run the cell.\n",
    "(Hint: if you're seeing something else, did you specify shuffling vs not shuffling option correctly?)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_images, val_labels = next(iter(val_loader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('images.shape:', val_images.shape)\n",
    "print('labels.shape:', val_labels.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(val_images[1].reshape((3, 32, 32)).permute((1,2,0)).numpy())\n",
    "print('label:', classes[val_labels[1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 The Forward Pass Implementation (2 pts)\n",
    "\n",
    "In this question we will implement a two-layered a neural network architecture as well as the loss function to train it. For this question, complete the required code in `solution.py`. Refer to the comments in the code to the exact places where you need to fill in the code.\n",
    "Load the code cell in this notebook to verify your answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Model Architecture\n",
    "\n",
    "The diagram below shows the visualization of the model architecture. Please note that the diagram only shows the schematic of the layers, the number of units are *not* representative of the actual sizes of the input, output, and hidden layers.\n",
    "\n",
    "<img src=\"AS7NNetwork.png\" alt=\"Model Architecture\" width=584/>\n",
    "\n",
    "Our 2-layer neural network has an input layer and two model layers: a hidden and an output layer. \n",
    "\n",
    "The hidden layer consists of 50 units. The input layer and the hidden layer are connected via linear weight matrix $W^{(1)}$ and the bias term $b^{(1)}$. The parameters $W^{(1)}$ and $b^{(1)}$ are to be learnt during training. A linear operation is performed, $W^{(1)}x + b^{(1)}$, resulting in a vector $z^{(2)}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is then followed by a ReLU non-linear activation, applied element-wise on each unit, resulting in the activations $a^{(2)} = \\text{ReLU}(z^{(2)})$.\n",
    "As you know, the ReLU function has the following form:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{1}\n",
    "\\text{ReLU}(u) = \\begin{cases} \n",
    "          u, & u \\ge 0 \\\\\n",
    "          0, & u < 0\n",
    "       \\end{cases}\n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A similar linear operation is performed on $a^{(2)}$, resulting in $z^{(3)} = W^{(2)}a^{(2)} + b^{(2)}$; it is followed by the softmax activation to result in $a^{(3)} =\\text{softmax}(z^{(3)})$. The softmax function is defined\n",
    "by:\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{2}\n",
    "\\text{softmax}(u_{(i)}) = \\frac{\\exp^{u_{i}}}{\\sum_{j}\\exp^{u_{j}}}\n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In equation form, our network is as described below:\n",
    "\n",
    "\\begin{align}\\tag{3}\n",
    "a^{(1)} &= x \\\\\n",
    "\\tag{4}\n",
    "z^{(2)} &= W^{(1)}a^{(1)} +b^{(1)}\\\\\n",
    "\\tag{5}\n",
    "a^{(2)} &=\\text{ReLu}(z^{(2)})\\\\\n",
    "\\tag{6}\n",
    "z^{(3)} &= W^{(2)}a^{(2)} +b^{(2)}\\\\\n",
    "\\tag{7}\n",
    "f_{\\theta}(x) &:= a^{(3)} =\\text{softmax}(z^{(3)})\n",
    "\\end{align}\n",
    "\n",
    "The network takes as input a flattened vector and outputs a vector where each entry in the output $f_{k}(x)$ representing the probability of image $x$ corresponding to the class $k$. We indicate all the network parameters by $θ = (W^{(1)},b^{(1)},W^{(2)},b^{(2)})$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the neural network to learn the parameters $\\theta = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})$ to fit to the given data and label, we minimize the loss function. A popular choice of the loss function for training a neural network for a multi-class classification task is the cross-entropy loss. \n",
    "\n",
    "For a single input sample $x_{i}$, with label $y_{i}$, the loss function is defined as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{8}\n",
    "J(\\theta, x_{i}, y_{i}) &= −\\log P(Y = y_{i}, X = x_{i})\\\\\n",
    "\\tag{9}\n",
    "&= − \\log f_{\\theta}(x_{i})_{y_{i}}\\\\\n",
    "\\tag{10}\n",
    "&= − \\log\\text{softmax}(z^{(3)})_{y_{i}}\\\\\n",
    "\\tag{11}\n",
    "J(\\theta, x_{i}, y_{i}) &= −\\log \\bigg[ \\frac{\\exp^{z^{(3)}_{y_{i}}}}{\\sum^{K}_{j}\\exp^{z^{(3)}_{j}}} \\bigg]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Averaging over the whole training set, we get:\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{12}\n",
    "J(\\theta, \\{x_{i}, y_{i}\\}^{N}_{i = 1}) &= \\frac{1}{N} \\sum^N_{i = 1} −\\log \\bigg[ \\frac{\\exp^{z^{(3)}_{y_{i}}}}{\\sum^{K}_{j}\\exp^{z^{(3)}_{j}}} \\bigg]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $K$ is the number of classes. Note that if the model has perfectly fitted to the data (i.e. $f_{\\theta}^{k}(x_{i}) = 1$ whenever $x_{i}$ belongs to class $k$ and $0$ otherwise), then $J$ attains the minimum of $0$.\n",
    "\n",
    "Apart from trying to correctly predict the label, we have to prevent overfitting the model to the current training data for better generalization to unseen data. We add an $L_{2}$ regularisation term over the model parameters $\\theta$. Specifically, the loss function is defined by:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{13}\n",
    "\\tilde{J}(\\theta) &= \\frac{1}{N} \\sum^N_{i = 1} −\\log \\bigg[ \\frac{\\exp^{z^{(3)}_{y_{i}}}}{\\sum^{K}_{j}\\exp^{z^{(3)}_{j}}} \\bigg]\n",
    "+ \\lambda \\big( \\Vert W^{(1)}\\Vert^{2}_{2} + \\Vert W^{(2)} \\Vert^{2}_{2} \\big)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $\\Vert \\cdot \\Vert^{2}_{2}$ is the squared $L_{2}$ norm. For example,\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{14}\n",
    "\\Vert W^{(1)} \\Vert^{2}_{2} &= \\sum^{R}_{p=1}\\sum^{S}_{q=1}{W^{(1)}_{pq}}^{2}, \\text{where }R = \\text{hidden size},S = \\text{input size}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Implement the code in `solution.py`** for the *forward pass* of model up to the *loss function* as described above. You are required to implement **Eq. 3 to 7** as well as **Eq. 11 - 13**.\n",
    "\n",
    "To be able to train the above model on large datasets, with larger layer widths, the code has to be very efficient. To do this you should **avoid** using any python `for` loops in the forward pass and instead use matrix/vector multiplication in the `numpy` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Run the cells below to verify your answer. Do not change any of the codes below. If you encounter errors, revise your work in `solution.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from solution import NeuralNetworkModel\n",
    "from utils import correct_scores, correct_loss\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "num_samples = 5\n",
    "\n",
    "toy_net = NeuralNetworkModel(input_size, hidden_size, num_classes)\n",
    "scores = toy_net.loss(val_images[:num_samples,:])\n",
    "\n",
    "assert np.allclose(correct_scores(), scores)\n",
    "\n",
    "print(f'Difference between your scores and correct scores:\\n'\n",
    "      f'{np.sum(np.abs(scores - correct_scores()))}\\n'\n",
    "      f'Scores difference < 1e-5: {np.sum(np.abs(scores - correct_scores())) < 1e-5}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, _ = toy_net.loss(val_images[:num_samples,:], val_labels[:num_samples], reg=0.05)\n",
    "\n",
    "print(f'Correct loss: {correct_loss()}\\n'\n",
    "      f'Difference between your loss and correct loss:\\n'\n",
    "      f'{np.sum(np.abs(loss - correct_loss()))}\\n'\n",
    "      f'Loss difference < 1e-5: {np.sum(np.abs(loss - correct_loss())) < 1e-5}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Backpropagation (4 pts + 2 Bonus pts)\n",
    "\n",
    "When we train our model, we are trying to solve the following minimization function via stochatic gradient descent:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\tag{15}\n",
    "\\min_{\\theta} \\tilde J (\\theta) \n",
    "\\end{align}$$\n",
    "\n",
    "To find the gradients $\\nabla_{\\theta} \\tilde J(\\theta)$, we perform backpropagation from the output layer to the parameters $\\theta$ at different layers. Backpropagation is simply a sequential application of chain rule as covered in the lecture and your course materials. For each parameter $\\theta$ in the model we want to compute the effect that parameter has on the loss, we compute the derivatives of the loss w.r.t each model parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1 Gradient Calculation (2 pts + 2 Bonus pts)\n",
    "\n",
    "Complete 2 of the following derivative calculations. For 2 Bonus points, complete the rest of them.\n",
    "\n",
    "1. Verify that the loss function defined in **Eq.12** has the gradient w.r.t $ z^{(3)}$ as below (i.e. derive the following gradient):\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{16}\n",
    "\\frac{\\delta J}{\\delta z^{(3)}}(\\{x_i,y_i\\}^{N}_{i=1}) &= \\dfrac{1}{N}\\text{softmax}(z^{(3)}) - \\Delta \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\Delta$ is a matrix of $N × K$ dimensions with:\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{17}\n",
    "\\Delta_{i,j} = \\begin{cases} \n",
    "          1, & \\text{if }  y_{i} = j \\\\\n",
    "          0, & \\text{otherwise}\n",
    "       \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "2. Verify that the partial derivative of the loss w.r.t $W^{(2)}$ is:\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{18}\n",
    "\\frac{\\delta J}{\\delta W^{(2)}}(\\{x_i,y_i\\}^{N}_{i=1}) &= \\frac{\\delta J}{\\delta z^{(3)}} \\cdot \\frac{\\delta z^{(3)}}{\\delta W^{(2)}} \\\\\n",
    "\\tag{19}\n",
    "&= \\dfrac{1}{N} \\big(\\text{softmax}(z^{(3)}) - \\Delta \\big) \\cdot a^{(2)}\n",
    "\\end{align}\n",
    "\n",
    "3. Verify that the regularized loss in **Eq. 13** has the following derivative w.r.t $W^{(2)}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\tag{20}\n",
    "\\frac{\\delta \\tilde J}{\\delta W^{(2)}} = \\dfrac{1}{N} \\big(\\text{softmax}(z^{(3)}) - \\Delta \\big) \\cdot a^{(2)} + 2\\lambda W^{(2)}\n",
    "\\end{align}\n",
    "\n",
    "4. Dervive the expressions for the derivatives of the regularized loss in *Eq.13* w.r.t *$W^{(1)}, b^{(1)}, b^{(2)}$*.  These are referred to as Eq. 21-23 in `solution.py`. You should at least try to complete this as you will need to come up with the formulas for these derivatives to complete the next section, 2.3.2.\n",
    "\n",
    "\n",
    "**Note:** Activation functions are applied *element-wise*, so the derivative of an activation function has the shape as the original input to the activation function. E.g. if we have $f(a)$, where $a \\in \\mathbb{R}^{n \\times m}$, then $f(a) \\in \\mathbb{R}^{n \\times m}$ and $(\\nabla_{a} f(a)) \\in \\mathbb{R}^{n \\times m}$. Hence, the derivative of the activation function is also applied **element-wise**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 <font color=\"red\"> Answer </font>\n",
    "\n",
    "1. \n",
    "2.\n",
    "3.\n",
    "4."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2 Backpropagation Implementation (2 pts)\n",
    "\n",
    "Using the expressions you obtained for the derivatives of the loss w.r.t model parameters, **implement** the back-propagation algorithm in the file `solution.py`. (i.e. **Eq. 16, 17, 19, 20, 21-23**)\n",
    "Note that even though normally you would use PyTorch's autograd for this, you should write the optimization formulas without it here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from solution import NeuralNetworkModel\n",
    "from utils import eval_numerical_gradient, rel_error, init_toy_data\n",
    "\n",
    "# computing the numerical gradient for W1 and b1 on the actual data would take way too long with the eval_numerical_gradient function\n",
    "# hence the toy data\n",
    "X_sm, y_sm = init_toy_data()\n",
    "toy_net_small = NeuralNetworkModel(input_size=4, hidden_size=10, output_size=3, std=1e-1)\n",
    "sm_loss, sm_grads = toy_net_small.loss(X_sm, y_sm, reg=0.05)\n",
    "\n",
    "for param_name in sm_grads:\n",
    "    f = lambda W: toy_net_small.loss(X_sm, y_sm, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, toy_net_small.params[param_name], verbose=False)\n",
    "    print(f'{param_name} max relative error: {rel_error(param_grad_num, sm_grads[param_name])}; '\n",
    "          f'error < 1e-8: {rel_error(param_grad_num, sm_grads[param_name]) < 1e-8}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
